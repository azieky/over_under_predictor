{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over Under Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "import math\n",
    "import requests\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Gambling Statistics from ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/spreadspoke_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all rows not contains an Over/Under Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['over_under_line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the \"Game Spread\" to an Positive number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"spread_favorite\"] = abs(df['spread_favorite'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add \"Total\" column for the Total Amount of Points Scored in a given game. Using Away Points Score  + Home Points Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total'] = df['score_home'] + df['score_away'] \n",
    "# Change to an Interger\n",
    "df = df.astype({'total': 'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop all Rows without an Over/Under Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.over_under_line != ' ']\n",
    "# Change to a Float\n",
    "df = df.astype({'over_under_line': 'float'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Target Variable, Over/Under Result Column\n",
    "* ##### 'Over' if the Total is greater than the Over/Under Line \n",
    "* ##### 'Under' if the Total is less than the Over/Under Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['over_under_result'] = np.where(df['total'] > df['over_under_line'], 'Over', 0)\n",
    "df['over_under_result'] = np.where(df['total'] == df['over_under_line'], np.nan, df['over_under_result'])\n",
    "df['over_under_result'] = np.where(df['total'] < df['over_under_line'], 'Under', df['over_under_result'])\n",
    "# Drop all 'Pushed/Tied' Totals\n",
    "df = df.dropna(subset=['over_under_result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10052, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update 'Washington Redskins' team name, to 'Washington Football team'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['team_home'] = np.where(df['team_home'] == 'Washington Redskins', 'Washington Football Team', df['team_home'])\n",
    "df['team_away'] = np.where(df['team_away'] == 'Washington Redskins', 'Washington Football Team', df['team_away'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct all Alternative Names of Stadiums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stadium_name'] = df['stadium']\n",
    "df = df.drop(columns=['stadium'])\n",
    "\n",
    "# Create List of Stadium Names\n",
    "stadium_list = list(df['stadium_name'])\n",
    "\n",
    "# Create List of Hard Rock Stadium alternative stadium names\n",
    "hard_rock_stadium_alternates = ['Joe Robbie Stadium','Pro Player Stadium','Dolphin Stadium']\n",
    "\n",
    "# Create for loop that Changes all alternative name to 'Hard Rock Stadium'\n",
    "for i, stadium in enumerate(stadium_list):\n",
    "    if stadium in hard_rock_stadium_alternates:\n",
    "        stadium_list[i] = 'Hard Rock Stadium'\n",
    "        \n",
    "# Update Column Row with new stadium list with Hard Rock Stadium instead of Alternatives\n",
    "df['stadium_name'] = stadium_list\n",
    "\n",
    "# Change other independent events of Stadium name changes\n",
    "df['stadium_name'] = np.where(df['stadium_name'] == 'Tampa Stadium', 'Raymond James Stadium',df['stadium_name'])\n",
    "df['stadium_name'] = np.where(df['stadium_name'] == 'Alltel Stadium', 'TIAA Bank Field',df['stadium_name'])\n",
    "df['stadium_name'] = np.where(df['stadium_name'] == 'Jack Murphy Stadium', 'Qualcomm Stadium',df['stadium_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Stadium Data from ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stadium_df = pd.read_csv('data/nfl_stadiums.csv')\n",
    "\n",
    "# Merge Stadium Data to the Final DataFrame\n",
    "df = df.merge(stadium_df,on='stadium_name',how='left')\n",
    "\n",
    "#Drop Uneeded Columns\n",
    "df = df.drop(columns=['stadium_close','NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Stadium Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually Add Stadium Locations\n",
    "df['stadium_location'] = np.where(df['stadium_name'] == 'FedEx Field', 'Landover, MD',df['stadium_location'])\n",
    "df['stadium_location'] = np.where(df['stadium_name'] == 'TIAA Bank Field', 'Jacksonville, FL',df['stadium_location'])\n",
    "\n",
    "# Drop all Nulls\n",
    "df = df.dropna(subset=['stadium_location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Games Played before 1978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.schedule_season > 1978]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct the Data the Stadium Opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FedEx Field         187\n",
       "TIAA Bank Field      17\n",
       "Rogers Centre         6\n",
       "Rose Bowl             4\n",
       "Alamo Dome            3\n",
       "Stanford Stadium      1\n",
       "Name: stadium_name, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['stadium_open'].isna()].stadium_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change Open Years\n",
    "df['stadium_open'] = np.where(df['stadium_name'] == 'FedEx Field', 1997,df['stadium_open'])\n",
    "df['stadium_open'] = np.where(df['stadium_name'] == 'TIAA Bank Field', 1994,df['stadium_open'])\n",
    "df['stadium_open'] = np.where(df['stadium_name'] == 'Rose Bowl', 1921,df['stadium_open'])\n",
    "df['stadium_open'] = np.where(df['stadium_name'] == 'Alamo Dome', 1993,df['stadium_open'])\n",
    "df['stadium_open'] = np.where(df['stadium_name'] == 'Stanford Stadium', 1921,df['stadium_open'])\n",
    "\n",
    "df = df[df.stadium_name != 'Rogers Centre']\n",
    "\n",
    "# Change to Integer\n",
    "df = df.astype({'stadium_open': 'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the Type of Stadium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stadium_type'] = np.where(df['stadium_name'] == 'FedEx Field', 'outdoor',df['stadium_type'])\n",
    "df['stadium_type'] = np.where(df['stadium_name'] == 'TIAA Bank Field', 'outdoor',df['stadium_type'])\n",
    "df['stadium_type'] = np.where(df['stadium_name'] == 'Stanford Stadium', 'outdoor',df['stadium_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Zip Codes for all Stadiums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Zipcodes by splitting last 5 elements from Address Values\n",
    "df['zipcode'] = df['stadium_address'].str[-5:]\n",
    "\n",
    "# Adding Zip Codes to Stadiums Without Address\n",
    "df['zipcode'] = np.where(df['stadium_name'] == 'Rose Bowl', 91103,df['zipcode'])\n",
    "df['zipcode'] = np.where(df['stadium_name'] == 'Stanford Stadium', 94305,df['zipcode'])\n",
    "df['zipcode'] = np.where(df['stadium_name'] == 'FedEx Field', 20785,df['zipcode'])\n",
    "df['zipcode'] = np.where(df['stadium_name'] == 'TIAA Bank Field', 32202,df['zipcode'])\n",
    "df['zipcode'] = np.where(df['stadium_name'] == 'Mercedes-Benz Stadium', 30313,df['zipcode'])\n",
    "df['zipcode'] = np.where(df['stadium_name'] == 'SoFi Stadium', 90301,df['zipcode'])\n",
    "df['zipcode'] = np.where(df['stadium_name'] == 'Allegiant Stadium', 89118,df['zipcode'])\n",
    "\n",
    "# Drop Stadiums outside the U.S.\n",
    "df = df[df.stadium_name != 'Wembley Stadium']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the Surface Type for every Stadium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change other names for Turf and Grass\n",
    "df['stadium_surface'] = np.where(df['stadium_surface'] == 'Hellas Matrix Turf', 'FieldTurf',df['stadium_surface'])\n",
    "df['stadium_surface'] = np.where(df['stadium_surface'] == 'Grass, Turf (1971-1974)', 'Grass',df['stadium_surface'])\n",
    "\n",
    "# Make corresponding list from the stadium name & stadium surface columns\n",
    "stadium_list = list(df['stadium_name'])\n",
    "surface_list = list(df['stadium_surface'])\n",
    "# Create list for All Stadiums with Turf Surfaces\n",
    "turf_stadiums = ['Giants Stadium','Texas Stadium','Hubert H. Humphrey Metrodome','RCA Dome','Veterans Stadium',\n",
    "                'Foxboro Stadium','Pontiac Silverdome','Three Rivers Stadium','Edward Jones Dome','Cinergy Field',\n",
    "                'Seattle Kingdome','Houston Astrodome','Busch Memorial Stadium','Mall of America Field',\n",
    "                'Husky Stadium',]\n",
    "# Create for loops to add Correct Surface for every Stadium in turf_stadium list\n",
    "for i, stadium in enumerate(stadium_list):\n",
    "    # Get the Index for the Stadium if the Stadium is in Turf Stadium List\n",
    "    if stadium in turf_stadiums:\n",
    "        surface_list[i] = 'FieldTurf'\n",
    "    # Use the index to change the corresponding Surface Type\n",
    "df['stadium_surface'] = surface_list\n",
    "\n",
    "# Change all the Stadiums that dont have Turf to Grass values\n",
    "df['stadium_surface'] = np.where(df['stadium_surface'] == 'FieldTurf', 'FieldTurf', 'Grass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Stadium Capacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually Import Each Stadium Capacity\n",
    "\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Giants Stadium',80242,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Candlestick Park',69732,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Sun Life Stadium',64767,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Texas Stadium',65675,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Hubert H. Humphrey Metrodome', 64121,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'RCA Dome',60567,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Veterans Stadium',65352,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'FedEx Field', 82000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Foxboro Stadium',60292,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Pontiac Silverdome',80311,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Mile High Stadium', 75000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Three Rivers Stadium',59000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Edward Jones Dome', 67277,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Cinergy Field', 59754,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Seattle Kingdome',66000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == \"Houlihan's Stadium\", 50000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Houston Astrodome', 65000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'RFK Memorial Stadium',45596,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Cleveland Municipal Stadium', 81000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Anaheim Stadium', 69008,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Atlanta-Fulton County Stadium', 60606,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Busch Memorial Stadium', 60000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Orange Bowl', 75000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Memorial Stadium (Baltimore)', 50000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Sun Devil Stadium', 53599,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Mall of America Field', 64121,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Metropolitan Stadium', 41200,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Wembley Stadium', 86000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Husky Stadium', 70000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'TCF Bank Stadium', 50805,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'TIAA Bank Field', 67814,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == \"Memorial Stadium (Champaign)\", 60670,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == \"Memorial Stadium (Clemson)\", 74000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Liberty Bowl Memorial Stadium', 58325,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Vanderbilt Stadium', 40550,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Rose Bowl', 92542,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == \"Tiger Stadium (LSU)\", 100000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Tulane Stadium', 70000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Stanford Stadium', 50000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Rice Stadium', 47000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Tulane Stadium', 70000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Stanford Stadium', 50000,df['stadium_capacity'])\n",
    "df['stadium_capacity'] = np.where(df['stadium_name'] == 'Rice Stadium', 47000,df['stadium_capacity'])\n",
    "\n",
    "# Take out comma's from the Stadium Capacity\n",
    "df['stadium_capacity'] = df.stadium_capacity.replace(',','', regex=True)\n",
    "\n",
    "df = df[df.stadium_name != 'Estadio Azteca']\n",
    "df = df[df.stadium_name != 'Twickenham Stadium']\n",
    "\n",
    "# Stadium Capacity to Integer\n",
    "df = df.astype({'stadium_capacity': 'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['stadium_capacity'].isna()].stadium_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Column for which Week of a Season a given game was played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 1-17 Regular Season Games\n",
    "# Week 18 and Above Playoff Games\n",
    "\n",
    "# Wildcard Playoff Games set as Week 18\n",
    "df['schedule_week'] = np.where(df['schedule_week'] == \"Wildcard\", 18,df['schedule_week'])\n",
    "df['schedule_week'] = np.where(df['schedule_week'] == \"WildCard\", 18,df['schedule_week'])\n",
    "# Divional Playoff Games set as Week 19\n",
    "df['schedule_week'] = np.where(df['schedule_week'] == 'Division', 19,df['schedule_week'])\n",
    "# Conference Championship Games set as Week 20\n",
    "df['schedule_week'] = np.where(df['schedule_week'] == 'Conference', 20,df['schedule_week'])\n",
    "# Superbowl Championship Games set as Week 21\n",
    "df['schedule_week'] = np.where(df['schedule_week'] == 'Superbowl', 21,df['schedule_week'])\n",
    "df['schedule_week'] = np.where(df['schedule_week'] == 'SuperBowl', 21,df['schedule_week'])\n",
    "\n",
    "# Change Week Number to Integers \n",
    "df = df.astype({'schedule_week': 'int'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Day of the Week a game was played "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the string to datetime format\n",
    "import datetime\n",
    "df['schedule_date'] = pd.to_datetime(df['schedule_date'])\n",
    "\n",
    "\n",
    "# Get Day of the Week Game was Played\n",
    "# 0 = Monday, \n",
    "# 1 = Tuesday\n",
    "# 2 = Wednesday\n",
    "# 3 = Thursday\n",
    "# 4 = Friday\n",
    "# 5 = Saturday\n",
    "# 6 = Sunday\n",
    "df['weekday'] = df['schedule_date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Distance Between Home & Away Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with creating a column for Away Team Zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get an accurate Zipcode, get dataframe from early in the week & not a neutral stadium.\n",
    "early = df[(df.schedule_week < 4)&(df.stadium_neutral == False)]\n",
    "\n",
    "# Get a Dataframe of just the Teams and Their Corresponding Zipcodes\n",
    "zip_df = early[['team_home', 'zipcode']]\n",
    "\n",
    "\n",
    "# Change Column Names to use as Zipcode when Teams Away \n",
    "zip_df.columns = ['team_away','zipcode_away']\n",
    "\n",
    "# Drop Duplicates so Teams only have one Zipcode per Season\n",
    "zip_df = zip_df.drop_duplicates(subset = 'team_away')\n",
    "\n",
    "## Merge Away Zip Codes to Original Dataset ##\n",
    "df = df.merge(zip_df,how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Function to get distance between Two Zip Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install The Needed Packages ##\n",
    "#!pip install uszipcode\n",
    "#!pip install mpu\n",
    "\n",
    "from uszipcode import SearchEngine\n",
    "import mpu\n",
    "\n",
    "# Zipcode Reader note Reading zipcode of Tempe,AZ so changed to Scottsdale,AZ\n",
    "df['zipcode'] = np.where(df['zipcode'] == '85287','85054',df['zipcode'])\n",
    "df['zipcode_away'] = np.where(df['zipcode_away'] == '85287','85054',df['zipcode_away'])\n",
    "\n",
    "# Instantiate Zip Code Reader\n",
    "search = SearchEngine(simple_zipcode=True)\n",
    "\n",
    "# Define Function that Takes two Zip Codes and Returns Distance between the Zip Codes\n",
    "def get_dist(zipcode_1,zipcode_2):\n",
    "    # Get Zip Code 1 Latitude and Longitude\n",
    "    zip1 = search.by_zipcode(zipcode_1)\n",
    "    lat1 = zip1.lat\n",
    "    long1 = zip1.lng\n",
    "    # Get Zip Code 2 Latitude and Longitude\n",
    "    zip2 = search.by_zipcode(zipcode_2)\n",
    "    lat2 = zip2.lat\n",
    "    long2 = zip2.lng\n",
    "    # Return Distance\n",
    "    return mpu.haversine_distance((lat1,long1),(lat2,long2))\n",
    "\n",
    "# Create Column Taking in the Zip Code of the Home Team and Zip Code of the Away Team. \n",
    "# Returning a column returning the distance between the Two Zip Codes\n",
    "df['dist_diff'] = df[['zipcode', 'zipcode_away']].apply(lambda x: get_dist(*x), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Team Data from ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Webscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Scrape Settin Up Team Statistics Dataframe\n",
    "# URL to Pro Football Reference where there Data is located\n",
    "url = 'https://www.pro-football-reference.com/years/'+'1978'+'/'\n",
    "# Retrieve the First Dataframe containing AFC Conference Data\n",
    "afc_df = pd.read_html(url)[0]\n",
    "# Set the Year of the Season\n",
    "afc_df['Season_Year'] = '1978'\n",
    "# Retrieve the First Dataframe containing NFC Conference Data\n",
    "nfc_df = pd.read_html(url)[1]\n",
    "# Set the Year of the Season\n",
    "nfc_df['Season_Year'] = '1978'\n",
    "# Merge the Two Dataframes\n",
    "df_1 = [nfc_df,afc_df]\n",
    "team_df = pd.concat(df_1)\n",
    "team_df = team_df.reset_index(drop=True)\n",
    "# Delete Uneeded AFC and NFC Rows\n",
    "team_df = team_df[~team_df.Tm.str.contains(\"AFC\")]\n",
    "team_df = team_df[~team_df.Tm.str.contains(\"NFC\")]\n",
    "# Delete Uneeded Marks on Team Names\n",
    "team_df['Tm'] = team_df['Tm'].str.replace('+','')\n",
    "team_df['Tm'] = team_df['Tm'].str.replace('*','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the Season needed to iterate through URL\n",
    "years = list(map(str,df.schedule_season.unique()))\n",
    "\n",
    "# Set up loop to retrieve all data for Needed Years(Seasons)\n",
    "for year in years:\n",
    "    # Set up url with corrisponding Year(Season) for the destination of the data\n",
    "    url = 'https://www.pro-football-reference.com/years/'+year+'/'\n",
    "    # get afc dataframe and add year\n",
    "    afc_df = pd.read_html(url)[0]\n",
    "    afc_df['Season_Year'] = year\n",
    "    # get nfc dataframe and add year\n",
    "    nfc_df = pd.read_html(url)[1]\n",
    "    nfc_df['Season_Year'] = year\n",
    "    # Combine dataframes\n",
    "    df_1 = [nfc_df,afc_df]\n",
    "    df_1 = pd.concat(df_1)\n",
    "    team_df = pd.merge(team_df, df_1,how = 'outer')\n",
    "    # Clean Master Dataframe\n",
    "    team_df = team_df.reset_index(drop=True)\n",
    "    team_df = team_df[~team_df.Tm.str.contains(\"AFC\")]\n",
    "    team_df = team_df[~team_df.Tm.str.contains(\"NFC\")]\n",
    "    team_df['Tm'] = team_df['Tm'].str.replace('+','')\n",
    "    team_df['Tm'] = team_df['Tm'].str.replace('*','')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update NFL team names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df['Tm'] = np.where(team_df['Tm'] == 'Washington Redskins', 'Washington Football Team',team_df['Tm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ready the Team Season data for Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert Columns to Float\n",
    "team_df[['W', 'L', 'T', 'W-L%', 'PF', 'PA', 'PD', 'MoV', 'SoS', 'SRS',\n",
    "       'OSRS', 'DSRS', 'Season_Year']] = team_df[['W', 'L', 'T', 'W-L%', 'PF', 'PA', 'PD', 'MoV', 'SoS', 'SRS',\n",
    "       'OSRS', 'DSRS', 'Season_Year']].astype(float)\n",
    "\n",
    "# Seperate Data into Two Dataframe for Home Team & Away Team\n",
    "home_team_df = team_df\n",
    "away_team_df = team_df\n",
    "\n",
    "# Add 1 to the Season Year because we are using previous season record\n",
    "home_team_df['schedule_season'] = home_team_df['Season_Year'] + 1 \n",
    "# No Value for Tied Games = 0\n",
    "home_team_df['T'] = home_team_df['T'].fillna(0)\n",
    "# Create a Points For per Games Column\n",
    "home_team_df['PF_per_game'] = home_team_df['PF']/(home_team_df['W'] + home_team_df['L'] + home_team_df['T'])\n",
    "# Create a Points Against per Games Column\n",
    "home_team_df['PA_per_game'] = home_team_df['PA']/(home_team_df['W'] + home_team_df['L'] + home_team_df['T'])\n",
    "# Drop Redundant Team Statistics\n",
    "home_team_df = home_team_df.drop(columns=['Season_Year', 'W', 'L', 'T','PF','PA'])\n",
    "\n",
    "# add 'home_' before all Home Team Columns\n",
    "col_names = list(home_team_df.columns)\n",
    "home_col_names = [\"home_\" + col for col in col_names]\n",
    "home_team_df.columns = home_col_names\n",
    "# Change 'home_tm' column name to corrispond with Final Dataset 'team_home'\n",
    "home_team_df['team_home'] = home_team_df['home_Tm']\n",
    "# Change 'home_schedule_season' column name to corrispond with Final Dataset 'schedule_season'\n",
    "home_team_df['schedule_season'] = home_team_df['home_schedule_season']\n",
    "# Drop Uneeded Columns\n",
    "home_team_df = home_team_df.drop(columns=['home_Tm','home_schedule_season'])\n",
    "\n",
    "# Left Merge Team Data to Final Dataframe. \n",
    "# Will merge on team_home and schedule_season\n",
    "final_df = df.merge(home_team_df,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reiterate the Same process for Away Team Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "away_team_df['schedule_season'] = away_team_df['Season_Year'] + 1\n",
    "away_team_df['T'] = away_team_df['T'].fillna(0)\n",
    "away_team_df['PF_per_game'] = away_team_df['PF']/(away_team_df['W'] + away_team_df['L'] + away_team_df['T'])\n",
    "away_team_df['PA_per_game'] = away_team_df['PA']/(away_team_df['W'] + away_team_df['L'] + away_team_df['T'])\n",
    "away_team_df = away_team_df.drop(columns=['Season_Year', 'W', 'L', 'T','PF','PA'])\n",
    "\n",
    "col_names = list(away_team_df.columns)\n",
    "away_col_names = [\"away_\" + col for col in col_names]\n",
    "away_team_df.columns = away_col_names\n",
    "away_team_df['team_away'] = away_team_df['away_Tm']\n",
    "away_team_df['schedule_season'] = away_team_df['away_schedule_season']\n",
    "away_team_df = away_team_df.drop(columns=['away_Tm','away_schedule_season'])\n",
    "\n",
    "final_df = final_df.merge(away_team_df,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop All null values for team names that didnt match Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.dropna(subset=['home_W-L%'])\n",
    "final_df = final_df.dropna(subset=['away_W-L%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Preseason Superbowl Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Webscraper to retrieve Preseason Superbowl Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url with the destination of the Needed Data\n",
    "url = \"https://www.pro-football-reference.com/years/2020/preseason_odds.htm#preseason_odds::none\"\n",
    "# Set up the Orginial Dataframe with 2020 Preseason Superbowl Odds\n",
    "superbowl_odds = pd.read_html(url)[0]\n",
    "superbowl_odds['Season_Year'] = '2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the years list to determine the data retrieved \n",
    "for year in years:\n",
    "    url = \"https://www.pro-football-reference.com/years/\"+year+\"/preseason_odds.htm\"\n",
    "    superbowl_odds_1 = pd.read_html(url)[0]\n",
    "    superbowl_odds_1['Season_Year'] = year\n",
    "    # Merge Superbowl Odds to original Superbowl Odds dataset\n",
    "    superbowl_odds = pd.merge(superbowl_odds, superbowl_odds_1,how = 'outer')\n",
    "\n",
    "# Drop Uneeded Columns\n",
    "superbowl_odds_df = superbowl_odds.drop(columns=['W/L O-U','Record'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix outdated team names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "superbowl_odds_df['Tm'] = np.where(superbowl_odds_df['Tm'] == 'Washington Redskins', 'Washington Football Team',superbowl_odds_df['Tm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperate dataframe into two matching Away Team and Home Team dataframs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change schedule_season to an integer to match dataframes\n",
    "superbowl_odds_df[['Season_Year']] = superbowl_odds_df[['Season_Year']].astype(float)\n",
    "\n",
    "# Split to Home and Away Teams\n",
    "home_superbowl_odds_df = superbowl_odds_df\n",
    "away_superbowl_odds_df = superbowl_odds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Superbowl Data to Merge into Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change home team column names to match Final Dataframe Column names\n",
    "home_superbowl_odds_df.columns = ['team_home', 'home_superbowl_odds','schedule_season']\n",
    "# Left Merge match team_home and 'schedule_season'\n",
    "final_df = final_df.merge(home_superbowl_odds_df,how='left')\n",
    "\n",
    "# Reiterate Process for the Away Team\n",
    "away_superbowl_odds_df.columns = ['team_away', 'away_superbowl_odds','schedule_season']\n",
    "final_df = final_df.merge(away_superbowl_odds_df,how='left')\n",
    "\n",
    "# Will Amend if there is Time, For now drop 2020 data\n",
    "final_df = final_df.dropna(subset=['home_superbowl_odds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Differences of Team Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets differnce in Team Data Statistics\n",
    "final_df['diff_W-L%'] = abs(final_df['home_W-L%'] - final_df['away_W-L%'])\n",
    "final_df['diff_PD'] = abs(final_df['home_PD']- final_df['away_PD'])\n",
    "final_df['diff_MoV'] = abs(final_df['home_MoV']- final_df['away_MoV'])\n",
    "final_df['diff_SoS'] = abs(final_df['home_SoS']- final_df['away_SoS'])\n",
    "final_df['diff_SRS'] = abs(final_df['home_SRS']- final_df['away_SRS'])\n",
    "final_df['diff_OSRS'] = abs(final_df['home_OSRS']- final_df['away_OSRS'])\n",
    "final_df['diff_PF_per_game'] = abs(final_df['home_PF_per_game']- final_df['away_PF_per_game'])\n",
    "final_df['diff_PA_per_game'] = abs(final_df['home_PA_per_game']- final_df['away_PA_per_game'])\n",
    "final_df['diff_superbowl_odds'] = abs(final_df['home_superbowl_odds']- final_df['away_superbowl_odds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Longitudes and Latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pgeocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgeocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zips = list(final_df.zipcode.unique())\n",
    "all_zips = [str(i) for i in all_zips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomi = pgeocode.Nominatim('us')\n",
    "long_lat_df = nomi.query_postal_code(all_zips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lat_df = long_lat_df.drop(columns =['country_code','place_name','state_name','state_code','county_name',\n",
    "                               'county_code','community_name','community_code','accuracy'])\n",
    "final_df = final_df.drop(columns =['LATITUDE','LONGITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lat_df.columns = ['zipcode','LATITUDE','LONGITUDE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, long_lat_df,how='inner',on=[\"zipcode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.sort_values(by='schedule_date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('data/final_df.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_UNIX(x):\n",
    "    unix = pd.to_datetime([x + ' 21:00:00']).astype(int) / 10**9\n",
    "    return int(unix[0])\n",
    "\n",
    "final_df['schedule_date'] = final_df['schedule_date'].dt.strftime('%Y-%m-%d')\n",
    "final_df['unix'] = final_df['schedule_date'].apply(get_UNIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['LATITUDE']=final_df['LATITUDE'].astype(str)\n",
    "final_df['LONGITUDE']= final_df['LONGITUDE'].astype(str)\n",
    "final_df['unix']= final_df['unix'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(latitude,longitude,unix):\n",
    "    url = \"https://dark-sky.p.rapidapi.com/\"+latitude+','+longitude+','+unix\n",
    "    headers = {\n",
    "    'x-rapidapi-key': \"ace28e75e5msh0810e63ff3d1c2dp16c863jsnf40bb24e6796\",\n",
    "    'x-rapidapi-host': \"dark-sky.p.rapidapi.com\"}\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    data = response.json()\n",
    "    weather = data['currently']\n",
    "    weather = pd.DataFrame(weather, index=[0])\n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_thousand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-63b3872b6a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweather_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_weather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'27.9625'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-82.4895'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'305067600'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_thousand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlatitude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LATITUDE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlongitude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LONGITUDE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0munix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'first_thousand' is not defined"
     ]
    }
   ],
   "source": [
    "weather_df = get_weather('27.9625','-82.4895','305067600')\n",
    "for index, row in first_thousand.iterrows():\n",
    "    latitude = row['LATITUDE']\n",
    "    longitude = row['LONGITUDE']\n",
    "    unix = row['unix']\n",
    "    weather_df2 = get_weather(latitude,longitude,unix)\n",
    "    weather_df = pd.concat([weather_df, weather_df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weather_df = weather_df.reset_index().drop([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weather_df = weather_df.reset_index()\n",
    "weather_df = weather_df.drop(columns = ['level_0','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_df.shape)\n",
    "weather_df\n",
    "#weather_df.to_csv('data/weather_df.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weather = weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weather = pd.read_csv('data/weather_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weather = final_weather.drop(columns = ['index','level_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf = final_df.merge(final_weather, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf = mergedDf.drop(columns = ['precipAccumulation','ozone','windGust','weather_temperature',\n",
    "                                   'weather_wind_mph','weather_humidity','weather_detail'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schedule_date                      0\n",
       "schedule_season                    0\n",
       "schedule_week                      0\n",
       "schedule_playoff                   0\n",
       "team_home                          0\n",
       "score_home                         0\n",
       "score_away                         0\n",
       "team_away                          0\n",
       "team_favorite_id                   0\n",
       "spread_favorite                    0\n",
       "over_under_line                    0\n",
       "stadium_neutral                    0\n",
       "total                              0\n",
       "over_under_result                  0\n",
       "stadium_name                       0\n",
       "stadium_location                   0\n",
       "stadium_open                       0\n",
       "stadium_type                       0\n",
       "stadium_address                    0\n",
       "stadium_weather_station_code       0\n",
       "stadium_weather_type               0\n",
       "stadium_capacity                   0\n",
       "stadium_surface                    0\n",
       "STATION                         2204\n",
       "ELEVATION                       2204\n",
       "zipcode                            0\n",
       "weekday                            0\n",
       "zipcode_away                       0\n",
       "dist_diff                          0\n",
       "home_W-L%                          0\n",
       "home_PD                            0\n",
       "home_MoV                           0\n",
       "home_SoS                           0\n",
       "home_SRS                           0\n",
       "home_OSRS                          0\n",
       "home_DSRS                          0\n",
       "home_PF_per_game                   0\n",
       "home_PA_per_game                   0\n",
       "away_W-L%                          0\n",
       "away_PD                            0\n",
       "away_MoV                           0\n",
       "away_SoS                           0\n",
       "away_SRS                           0\n",
       "away_OSRS                          0\n",
       "away_DSRS                          0\n",
       "away_PF_per_game                   0\n",
       "away_PA_per_game                   0\n",
       "home_superbowl_odds                0\n",
       "away_superbowl_odds                0\n",
       "diff_W-L%                          0\n",
       "diff_PD                            0\n",
       "diff_MoV                           0\n",
       "diff_SoS                           0\n",
       "diff_SRS                           0\n",
       "diff_OSRS                          0\n",
       "diff_PF_per_game                   0\n",
       "diff_PA_per_game                   0\n",
       "diff_superbowl_odds                0\n",
       "LATITUDE                           0\n",
       "LONGITUDE                          0\n",
       "unix                               0\n",
       "time                               0\n",
       "summary                           85\n",
       "icon                              84\n",
       "precipIntensity                   83\n",
       "precipProbability                 83\n",
       "precipType                      8168\n",
       "temperature                        1\n",
       "apparentTemperature                1\n",
       "dewPoint                           2\n",
       "humidity                           2\n",
       "pressure                           3\n",
       "windSpeed                          1\n",
       "windBearing                       28\n",
       "cloudCover                         2\n",
       "uvIndex                            2\n",
       "visibility                        10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "mergedDf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rain    1471\n",
       "snow     181\n",
       "Name: precipType, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedDf.precipType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf['summary'] = mergedDf.summary.fillna('None')\n",
    "mergedDf['icon'] = mergedDf.icon.fillna('None')\n",
    "mergedDf['precipIntensity'] = mergedDf.precipIntensity.fillna(0)\n",
    "mergedDf['precipProbability'] = mergedDf.precipProbability.fillna(0)\n",
    "mergedDf['precipType'] = mergedDf.precipProbability.fillna(0)\n",
    "mergedDf['windBearing'] = mergedDf.windBearing.fillna(0)\n",
    "mergedDf['visibility'] = mergedDf.visibility.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDf.to_csv('data/final_df.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
